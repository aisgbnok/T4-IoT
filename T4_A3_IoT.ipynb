{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpSZQMHwLTEP"
   },
   "source": [
    "# T4 - Identifying IoT Devices using Traffic Meta-data\n",
    "\n",
    "Semester 2221, CSEC 520/620, Team 4\\\n",
    "Assignment 4 - IoT Classification\\\n",
    "Due by November 15, 2022 11:59 PM EST.\\\n",
    "Accounts for 12% of total grade.\n",
    "\n",
    "This assignment includes some prewritten code for you to work with. This code is a re-implementation of the classification methods described in the 2018 paper [\"Classifying IoT Devices in Smart Environments Using Network Traffic Characteristics.\"](https://doi.org/10.1109/TMC.2018.2866249)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements\n",
    "\n",
    "- Python 3.7+\n",
    "- Download and unzip the materials & IoT dataset (for Google Colab only) using the following code-block..."
   ],
   "metadata": {
    "id": "Dxu6U3ilW4nf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILLfIEy_KL-8",
    "outputId": "05ce83ec-d093-428a-d45a-7871f063852c"
   },
   "outputs": [],
   "source": [
    "!gdown 199U9EjMlDsqfOTxaDRMZkALzMhJ1Hmsd\n",
    "!unzip A4_Materials.zip\n",
    "!unzip A4_Materials/iot_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OK2ZbrMu8dn"
   },
   "source": [
    "You should now have two new directories in your file lists.\n",
    "\n",
    "`/content/iot_data/*` contains the feature data for the samples in the IoT dataset.\n",
    "\n",
    "`/content/A4_Materials/*` contains additional resources...\n",
    "*   *08440758.pdf* is the research paper that describes the following classification pipeline and dataset in detail.\n",
    "*   *list_of_devices.txt* contains device name, MAC address, and connection type information for all devices in the dataset.\n",
    "*    *classify.py* and *requirements.txt* contain the Notebook code and necessary libs to execute the script locally.\n",
    "*    *iot_data.zip* is the compressed feature files for the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Preliminaries\n",
    "\n",
    "Before we can begin we have to define and import our various modules and libraries that we will depend on during execution."
   ],
   "metadata": {
    "id": "vSQ3x8qTW9B_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7Q-OIHLND0W"
   },
   "outputs": [],
   "source": [
    "# All primary imports\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Supress sklearn warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSSIU6lFPt8Q"
   },
   "source": [
    "Below are configurable hard-coded variables that used throughout the code...\n",
    "\n",
    "You are welcome to adjust these values however you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krwR5FzKNMK4"
   },
   "outputs": [],
   "source": [
    "# Seed Value\n",
    "# (ensures consistent dataset splitting between runs)\n",
    "SEED = 0\n",
    "\n",
    "# Default path for IoT data after unzipping the dataset (in Colab)\n",
    "ROOT = '/content/iot_data'\n",
    "\n",
    "# Percentage of samples to use for testing (feel free to change)\n",
    "SPLIT = 0.3\n",
    "\n",
    "# Port count threshold (e.g., discard port feature values that appear less than N times)\n",
    "PORT_COUNT = 10\n",
    "\n",
    "# Maximum and minimum number of samples to allow when loading each IoT device\n",
    "MAX_SAMPLES_PER_CLASS = 1000\n",
    "MIN_SAMPLES_PER_CLASS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data Loading & Processing"
   ],
   "metadata": {
    "id": "fth7HO6AW_A_"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUd9by6Fn1po"
   },
   "source": [
    "This starter code can be organized into three or four main sections: **(1)** *data loading & processing*, **(2)** *stage-0 classification of bags-of-words*, **(3)** *stage-1 classification using random forests*, and **(4)** *the main function* that glues everything together.\n",
    "\n",
    "The code will currently run as-is and correctly perform multi-class IoT device identification using the classification pipeline described in the assignment document. Additionally, each function's purpose and behavior is briefly described in its accompanying docstring. I recommend reading through all blocks of code to develop a general understanding of the way feature processing and classification is done.\n",
    "\n",
    "For this assignment, you will only *need* to modify code section **(3)**, but you also are free to adjust, modify, re-write any of the remaining code as you see fit to complete your analysis or to integrate better with your random forest implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVu_SZALO70g"
   },
   "outputs": [],
   "source": [
    "def load_data(root, min_samples, max_samples):\n",
    "    \"\"\"\n",
    "    Load json feature files produced from feature extraction.\n",
    "\n",
    "    The device label (MAC) is identified from the directory in which the feature file was found.\n",
    "    Returns x and y as separate multidimensional arrays.\n",
    "    The instances in x contain only the first 6 features.\n",
    "    The ports, domain, and cipher features are stored in separate arrays for easier process in stage 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "           Path to the directory containing samples.\n",
    "    min_samples : int\n",
    "                  The number of samples each class must have at minimum (else it is pruned).\n",
    "    max_samples : int\n",
    "                  Stop loading samples for a class when this number is reached.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features_misc : numpy array\n",
    "                    Traffic statistical features\n",
    "    features_ports : numpy array\n",
    "                     Vectorized word-bags (e.g., counts) for ports\n",
    "    features_domains : numpy array\n",
    "                       Vectorized word-bags (e.g., counts) for domains\n",
    "    features_ciphers : numpy array\n",
    "                       Vectorized word-bags (e.g., counts) for ciphers\n",
    "    labels : numpy array\n",
    "             (numerical) Labels for all samples in the dataset\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    x_p = []\n",
    "    x_d = []\n",
    "    x_c = []\n",
    "    y = []\n",
    "\n",
    "    port_dict = dict()\n",
    "    domain_set = set()\n",
    "    cipher_set = set()\n",
    "\n",
    "    # Create paths and do instance count filtering\n",
    "    f_paths = []\n",
    "    f_counts = dict()\n",
    "    for rt, dirs, files in os.walk(root):\n",
    "        for f_name in files:\n",
    "            path = os.path.join(rt, f_name)\n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            name = os.path.basename(path)\n",
    "            if name.startswith(\"features\") and name.endswith(\".json\"):\n",
    "                f_paths.append((path, label, name))\n",
    "                f_counts[label] = 1 + f_counts.get(label, 0)\n",
    "\n",
    "    # Load Samples\n",
    "    processed_counts = {label: 0 for label in f_counts.keys()}\n",
    "    for fpath in tqdm.tqdm(f_paths):  # Enumerate all sample files\n",
    "        path = fpath[0]\n",
    "        label = fpath[1]\n",
    "        if f_counts[label] < min_samples:\n",
    "            continue\n",
    "        if processed_counts[label] >= max_samples:\n",
    "            continue  # Limit\n",
    "        processed_counts[label] += 1\n",
    "        with open(path, \"r\") as fp:\n",
    "            features = json.load(fp)\n",
    "            instance = [features[\"flow_volume\"],\n",
    "                        features[\"flow_duration\"],\n",
    "                        features[\"flow_rate\"],\n",
    "                        features[\"sleep_time\"],\n",
    "                        features[\"dns_interval\"],\n",
    "                        features[\"ntp_interval\"]]\n",
    "            x.append(instance)\n",
    "            x_p.append(list(features[\"ports\"]))\n",
    "            x_d.append(list(features[\"domains\"]))\n",
    "            x_c.append(list(features[\"ciphers\"]))\n",
    "            y.append(label)\n",
    "            domain_set.update(list(features[\"domains\"]))\n",
    "            cipher_set.update(list(features[\"ciphers\"]))\n",
    "            for port in set(features[\"ports\"]):\n",
    "                port_dict[port] = 1 + port_dict.get(port, 0)\n",
    "\n",
    "    # Prune rarely seen ports\n",
    "    port_set = set()\n",
    "    for port in port_dict.keys():\n",
    "        if port_dict[port] > PORT_COUNT:  # Filter out ports that are rarely seen to reduce feature dimensionality\n",
    "            port_set.add(port)\n",
    "\n",
    "    # Map to word-bag\n",
    "    print(\"Generating word-bags ... \")\n",
    "    for i in tqdm.tqdm(range(len(y))):\n",
    "        x_p[i] = list(map(lambda x: x_p[i].count(x), port_set))\n",
    "        x_d[i] = list(map(lambda x: x_d[i].count(x), domain_set))\n",
    "        x_c[i] = list(map(lambda x: x_c[i].count(x), cipher_set))\n",
    "\n",
    "    return np.array(x).astype(float), np.array(x_p), np.array(x_d), np.array(x_c), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Stage-0 Classification of Bags-of-Words"
   ],
   "metadata": {
    "id": "FNXOitUCXExJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EPnl1blOyuV"
   },
   "outputs": [],
   "source": [
    "def classify_bayes(x_tr, y_tr, x_ts, y_ts):\n",
    "    \"\"\"\n",
    "    Use a multinomial naive bayes classifier to analyze the 'bag of words' seen in the ports/domain/ciphers features.\n",
    "    Returns the prediction results for the training and testing datasets as an array of tuples in which each row\n",
    "    represents a data instance and each tuple is composed as the predicted class and the confidence of prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_tr : numpy array\n",
    "           Array containing training samples.\n",
    "    y_tr : numpy array\n",
    "           Array containing training labels.\n",
    "    x_ts : numpy array\n",
    "           Array containing testing samples.\n",
    "    y_ts : numpy array\n",
    "           Array containing testing labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    c_tr : numpy array\n",
    "           Prediction results for training samples.\n",
    "    c_ts : numpy array\n",
    "           Prediction results for testing samples.\n",
    "    \"\"\"\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_tr, y_tr)\n",
    "\n",
    "    # Produce class and confidence for training samples\n",
    "    c_tr = classifier.predict_proba(x_tr)\n",
    "    c_tr = [(np.argmax(instance), max(instance)) for instance in c_tr]\n",
    "\n",
    "    # Produce class and confidence for testing samples\n",
    "    c_ts = classifier.predict_proba(x_ts)\n",
    "    c_ts = [(np.argmax(instance), max(instance)) for instance in c_ts]\n",
    "\n",
    "    return c_tr, c_ts\n",
    "\n",
    "\n",
    "def do_stage_0(xp_tr, xp_ts, xd_tr, xd_ts, xc_tr, xc_ts, y_tr, y_ts):\n",
    "    \"\"\"\n",
    "    Perform stage 0 of the classification procedure:\n",
    "        process each multinomial feature using naive bayes\n",
    "        return the class prediction and confidence score for each instance feature\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xp_tr : numpy array\n",
    "           Array containing training (port) samples.\n",
    "    xp_ts : numpy array\n",
    "           Array containing testing (port) samples.\n",
    "    xd_tr : numpy array\n",
    "           Array containing training (port) samples.\n",
    "    xd_ts : numpy array\n",
    "           Array containing testing (port) samples.\n",
    "    xc_tr : numpy array\n",
    "           Array containing training (port) samples.\n",
    "    xc_ts : numpy array\n",
    "           Array containing testing (port) samples.\n",
    "    y_tr : numpy array\n",
    "           Array containing training labels.\n",
    "    y_ts : numpy array\n",
    "           Array containing testing labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res_p_tr : numpy array\n",
    "               Prediction results for training (port) samples.\n",
    "    res_p_ts : numpy array\n",
    "               Prediction results for testing (port) samples.\n",
    "    res_d_tr : numpy array\n",
    "               Prediction results for training (domains) samples.\n",
    "    res_d_ts : numpy array\n",
    "               Prediction results for testing (domains) samples.\n",
    "    res_c_tr : numpy array\n",
    "               Prediction results for training (cipher suites) samples.\n",
    "    res_c_ts : numpy array\n",
    "               Prediction results for testing (cipher suites) samples.\n",
    "    \"\"\"\n",
    "    # Perform multinomial classification on bag of ports\n",
    "    res_p_tr, res_p_ts = classify_bayes(xp_tr, y_tr, xp_ts, y_ts)\n",
    "\n",
    "    # Perform multinomial classification on domain names\n",
    "    res_d_tr, res_d_ts = classify_bayes(xd_tr, y_tr, xd_ts, y_ts)\n",
    "\n",
    "    # Perform multinomial classification on cipher suites\n",
    "    res_c_tr, res_c_ts = classify_bayes(xc_tr, y_tr, xc_ts, y_ts)\n",
    "\n",
    "    return res_p_tr, res_p_ts, res_d_tr, res_d_ts, res_c_tr, res_c_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Stage-1 Classification using Random Forests"
   ],
   "metadata": {
    "id": "v5tU1-DBXGkZ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8D3Fj0EpLZ6"
   },
   "source": [
    "Your primary goal for this assignment is to implement your own versions of the decision tree and random forest algorithms to replace the scikit-learn implementation currently in use for stage-1 classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeMUBL7CCI25"
   },
   "outputs": [],
   "source": [
    "def gini_impurity(groups, classes):\n",
    "    n = sum(len(group) for group in groups)\n",
    "    impurity = 0.0\n",
    "    for group in groups:\n",
    "        if len(group) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            g_score = 0.0\n",
    "            for c in classes:\n",
    "                p = [row[-1] for row in group].count(c) / len(group)\n",
    "                g_score += p * p\n",
    "            impurity = impurity + (1 - g_score) * (len(group) / n)\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkYHnRDzVr4F"
   },
   "outputs": [],
   "source": [
    "def split(value, feature, data):\n",
    "    yes, no = [], []\n",
    "    for d in data:\n",
    "        if d[feature] < value:\n",
    "            no.append(d)\n",
    "        else:\n",
    "            yes.append(d)\n",
    "    return no, yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8N0gqBOL_94"
   },
   "outputs": [],
   "source": [
    "def splitter(data, classes):\n",
    "    index, value, score, groups = 0, 0, 2.0, None\n",
    "    i = 0\n",
    "    checked_feats = []\n",
    "    while i < len(data[0]) - 1:\n",
    "        for d in data:\n",
    "            if d[i] not in checked_feats:\n",
    "                checked_feats.append(d[i])\n",
    "                groups = split(d[i], i, data)\n",
    "                gini = gini_impurity(groups, classes)\n",
    "                if gini < score:\n",
    "                    index, value, score, groups = i, d[i], gini, groups\n",
    "        i += 1\n",
    "        checked_feats = []\n",
    "    return index, value, score, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71lGTdp8B0j3"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = data\n",
    "        self.split_on = None\n",
    "        self.split_threshold = None\n",
    "        self.purity = 2.0\n",
    "\n",
    "    def set_children(self, feature, value, left_node, right_node):\n",
    "        self.left = left_node\n",
    "        self.right = right_node\n",
    "        self.split_on = feature\n",
    "        self.split_threshold = value\n",
    "\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "\n",
    "    def get_purity(self):\n",
    "        return self.purity\n",
    "\n",
    "    def print_tree(self, layer=0):\n",
    "        if self.data is not None:\n",
    "            print(f\"Layer #{layer}: Feature {self.split_on} < {self.split_threshold}\")\n",
    "            layer = layer + 1\n",
    "            if self.left is not None:\n",
    "                print(f\"Left side of {self.split_on} < {self.split_threshold}\")\n",
    "                self.left.print_tree(layer)\n",
    "            if self.right is not None:\n",
    "                print(f\"Right side of {self.split_on} < {self.split_threshold}\")\n",
    "                self.right.print_tree(layer)\n",
    "\n",
    "    def predict(self, sample):\n",
    "        if self.left is not None and self.right is not None:\n",
    "            sample_val = sample[self.split_on]\n",
    "            if sample_val < self.split_threshold:\n",
    "                below = self.left.predict(sample)\n",
    "                return below\n",
    "            else:\n",
    "                below = self.right.predict(sample)\n",
    "                return below\n",
    "        else:\n",
    "            class_count = {}\n",
    "            for d in self.data:\n",
    "                if d[-1] not in class_count.keys():\n",
    "                    class_count[d[-1]] = 1\n",
    "                else:\n",
    "                    class_count[d[-1]] = class_count[d[-1]] + 1\n",
    "            return max(class_count, key=class_count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpMW1_KAFyhf"
   },
   "outputs": [],
   "source": [
    "def decision_tree(max_depth, min_node, data, classes, num_features, current_node=None, features_used=None):\n",
    "    if max_depth == 0:\n",
    "        return current_node\n",
    "    if current_node is None:\n",
    "        current_node = Node(data)\n",
    "    if features_used is None:\n",
    "        features_used = []\n",
    "    elif len(features_used) >= num_features:\n",
    "        return current_node\n",
    "    best_split = splitter(data, classes)\n",
    "    if best_split[2] > current_node.get_purity():\n",
    "        return current_node\n",
    "    elif len(best_split[3][0]) == 0 or len(best_split[3][1]) == 0:\n",
    "        return current_node\n",
    "    elif len(best_split[3][0]) < min_node or len(best_split[3][1]) < min_node:\n",
    "        return current_node\n",
    "    left_node = decision_tree(max_depth - 1, min_node, best_split[3][0], classes, num_features,\n",
    "                              current_node=current_node.get_left(), features_used=features_used)\n",
    "    right_node = decision_tree(max_depth - 1, min_node, best_split[3][1], classes, num_features,\n",
    "                               current_node=current_node.get_right(), features_used=features_used)\n",
    "    current_node.set_children(best_split[0], best_split[1], left_node, right_node)\n",
    "    if best_split[0] not in features_used:\n",
    "        features_used.append(best_split[0])\n",
    "    return current_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hI0Z9EVBP6R"
   },
   "outputs": [],
   "source": [
    "def random_forest(n_trees, data_frac, feature_sub_count, x_tr, y_tr):\n",
    "    reformed_x = np.zeros(shape=(len(x_tr), len(x_tr[0]) + 1))\n",
    "    i = 0\n",
    "    classes_y = []\n",
    "    while i < len(x_tr):\n",
    "        c = y_tr[i]\n",
    "        reformed_x[i] = np.append(x_tr[i], [c], 0)\n",
    "        if c not in classes_y:\n",
    "            classes_y.append(c)\n",
    "        i += 1\n",
    "    forest = []\n",
    "    slice_size = int(len(reformed_x) * data_frac)\n",
    "    indices = np.arange(len(reformed_x))\n",
    "    for n in range(n_trees):\n",
    "        x_sample = []\n",
    "        slice_index = np.random.choice(indices, size=slice_size, replace=False)\n",
    "        for si in slice_index:\n",
    "            x_sample.append(reformed_x[si])\n",
    "        bigtree = decision_tree(5, 10, x_sample, classes_y, feature_sub_count)\n",
    "        bigtree.print_tree()\n",
    "        forest.append(bigtree)\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2F5zXHn6kzA"
   },
   "outputs": [],
   "source": [
    "def rf_test(x_tr, y_tr, x_ts, y_ts):\n",
    "    \"\"\"\n",
    "    Performs testing on the random forest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_tr : numpy array\n",
    "           Array containing training samples.\n",
    "    y_tr : numpy array\n",
    "           Array containing training labels.\n",
    "    x_ts : numpy array\n",
    "           Array containing testing samples.\n",
    "    y_ts : numpy array\n",
    "           Array containing testing labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "           Accuracy percentage from test run.\n",
    "    \"\"\"\n",
    "    # Train Random Forest\n",
    "    correct_number = 0\n",
    "    trees = random_forest(1, .7, 1, x_tr, y_tr)\n",
    "    print(f'Length: {len(trees)}')\n",
    "\n",
    "    # Generate predictions for each sample\n",
    "    for sample in x_ts:\n",
    "        tree_predictions = {}\n",
    "\n",
    "        for tree in trees:\n",
    "            x_class = tree.predict(sample)\n",
    "            if x_class not in tree_predictions.keys():\n",
    "                tree_predictions[x_class] = 1\n",
    "            else:\n",
    "                tree_predictions[x_class] += 1\n",
    "\n",
    "        # Check if class prediction was correct\n",
    "        predicted_class = max(tree_predictions, key=tree_predictions.get)\n",
    "        x_dex = np.where(x_ts == sample)\n",
    "        actual_class = y_ts[x_dex[0]][0]\n",
    "\n",
    "        if predicted_class == actual_class:\n",
    "            correct_number += 1\n",
    "\n",
    "    return correct_number / len(x_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CmqCNZqH8HT"
   },
   "outputs": [],
   "source": [
    "def do_stage_1(x_tr, x_ts, y_tr, y_ts):\n",
    "    \"\"\"\n",
    "    Perform stage 1 of the classification procedure:\n",
    "        train a random forest classifier using the NB prediction probabilities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_tr : numpy array\n",
    "           Array containing training samples.\n",
    "    y_tr : numpy array\n",
    "           Array containing training labels.\n",
    "    x_ts : numpy array\n",
    "           Array containing testing samples.\n",
    "    y_ts : numpy array\n",
    "           Array containing testing labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred : numpy array\n",
    "           Final predictions on testing dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = rf_test(x_tr, y_tr, x_ts, y_ts)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Main Function"
   ],
   "metadata": {
    "id": "FethRFVnXNMD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Tw3vuseMdZy",
    "outputId": "66f61035-2a49-4ce8-9322-d1f8873cb9fd"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Load data, encode labels to numeric, and perform classification stages.\n",
    "    \"\"\"\n",
    "    # load dataset\n",
    "    print(\"Loading dataset ...\")\n",
    "    x, x_p, x_d, x_c, y = load_data(ROOT, min_samples=MIN_SAMPLES_PER_CLASS,\n",
    "                                    max_samples=MAX_SAMPLES_PER_CLASS)\n",
    "\n",
    "    # Encode labels into numerical values\n",
    "    print(\"\\nEncoding labels ...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "\n",
    "    print(\"Dataset statistics:\")\n",
    "    print(f\"\\t Classes: {len(le.classes_)}\")\n",
    "    print(f\"\\t Samples: {len(y)}\")\n",
    "    print(\"\\t Dimensions: \", x.shape, x_p.shape, x_d.shape, x_c.shape)\n",
    "\n",
    "    # Shuffle\n",
    "    print(f\"\\nShuffling dataset using seed {SEED} ...\")\n",
    "    s = np.arange(y.shape[0])\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(s)\n",
    "    x, x_p, x_d, x_c, y = x[s], x_p[s], x_d[s], x_c[s], y[s]\n",
    "\n",
    "    # Split\n",
    "    print(f\"Splitting dataset using train:test ratio of {1 - int(SPLIT * 10)}:{int(SPLIT * 10)} ...\")\n",
    "    cut = int(len(y) * SPLIT)\n",
    "    x_tr, xp_tr, xd_tr, xc_tr, y_tr = x[cut:], x_p[cut:], x_d[cut:], x_c[cut:], y[cut:]\n",
    "    x_ts, xp_ts, xd_ts, xc_ts, y_ts = x[:cut], x_p[:cut], x_d[:cut], x_c[:cut], y[:cut]\n",
    "\n",
    "    # Perform stage 0\n",
    "    print(\"\\nPerforming Stage 0 classification ...\")\n",
    "    p_tr, p_ts, d_tr, d_ts, c_tr, c_ts = do_stage_0(xp_tr, xp_ts, xd_tr, xd_ts, xc_tr, xc_ts, y_tr, y_ts)\n",
    "\n",
    "    # Build stage 1 dataset using stage 0 results\n",
    "    # NB predictions are concatenated to the statistical attributes processed from the flows\n",
    "    x_tr_full = np.hstack((x_tr, p_tr, d_tr, c_tr))\n",
    "    x_ts_full = np.hstack((x_ts, p_ts, d_ts, c_ts))\n",
    "\n",
    "    # Perform final classification\n",
    "    print(\"Performing Stage 1 classification ...\")\n",
    "    pred = do_stage_1(x_tr_full, x_ts_full, y_tr, y_ts)\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"\\nPrediction: {pred}\")\n",
    "    #print(classification_report(Y_ts, pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
