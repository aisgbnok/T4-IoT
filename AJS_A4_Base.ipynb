{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpSZQMHwLTEP"
   },
   "source": [
    "# Assignment 4 - Identifying IoT Devices using Traffic Meta-data\n",
    "\n",
    "This assignment includes some prewritten code for you to work with. This code a re-implementation of the classification methods described in the 2018 paper \"Classifying IoT Devices in Smart Environments Using Network Traffic Characteristics.\"\n",
    "\n",
    "Download and unzip the materials & IoT dataset (for Google Colab only) using the following code-block..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12800,
     "status": "ok",
     "timestamp": 1668706869252,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "ILLfIEy_KL-8",
    "outputId": "c1ed5a92-8e20-4242-ef63-971d1d526cce",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!gdown 199U9EjMlDsqfOTxaDRMZkALzMhJ1Hmsd\n",
    "!unzip A4_Materials.zip\n",
    "!unzip A4_Materials/iot_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OK2ZbrMu8dn"
   },
   "source": [
    "You should now have two new directories in your file lists.\n",
    "\n",
    "`/content/iot_data/*` contains the feature data for the samples in the IoT dataset.\n",
    "\n",
    "`/content/A4_Materials/*` contains additional resources...\n",
    "*   *08440758.pdf* is the research paper that describes the following classification pipeline and dataset in detail.\n",
    "*   *list_of_devices.txt* contains device name, MAC address, and connection type information for all devices in the dataset.\n",
    "*    *classify.py* and *requirements.txt* contain the Notebook code and necessary libs to execute the script locally.\n",
    "*    *iot_data.zip* is the compressed feature files for the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1364,
     "status": "ok",
     "timestamp": 1668706870606,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "r7Q-OIHLND0W"
   },
   "outputs": [],
   "source": [
    "# All primary imports\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Supress sklearn warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSSIU6lFPt8Q"
   },
   "source": [
    "Below are configurable hard-coded variables that used throughout the code...\n",
    "\n",
    "You are welcome to adjust these values however you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1668706870606,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "krwR5FzKNMK4"
   },
   "outputs": [],
   "source": [
    "# Seed Value\n",
    "# (ensures consistent dataset splitting between runs)\n",
    "SEED = 0\n",
    "\n",
    "# Default path for IoT data after unzipping the dataset (in Colab)\n",
    "ROOT = '/content/iot_data'\n",
    "\n",
    "# Percentage of samples to use for testing (feel free to change)\n",
    "SPLIT = 0.3\n",
    "\n",
    "# Port count threshold (e.g., discard port feature values that appear less than N times)\n",
    "PORT_COUNT = 10\n",
    "\n",
    "# Maximum and minimum number of samples to allow when loading each IoT device\n",
    "MAX_SAMPLES_PER_CLASS = 1000\n",
    "MIN_SAMPLES_PER_CLASS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUd9by6Fn1po"
   },
   "source": [
    "This starter code can be organized into three or four main sections: **(1)** *data loading & processing*, **(2)** *stage-0 classification of bags-of-words*, **(3)** *stage-1 classification using random forests*, and **(4)** *the main function* that glues everything together.\n",
    "\n",
    "The code will currently run as-is and correctly perform multi-class IoT device identification using the classification pipeline described in the assignment document. Additionally, each function's purpose and behavior is briefly described in its accompanying docstring. I recommend reading through all blocks of code to develop a general understanding of the way feature processing and classification is done.\n",
    "\n",
    "For this assignment, you will only *need* to modify code section **(3)**, but you also are free to adjust, modify, re-write any of the remaining code as you see fit to complete your analysis or to integrate better with your random forest implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1668706870607,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "aVu_SZALO70g"
   },
   "outputs": [],
   "source": [
    "def load_data(root, min_samples, max_samples):\n",
    "    \"\"\"\n",
    "    Load json feature files produced from feature extraction.\n",
    "\n",
    "    The device label (MAC) is identified from the directory in which the feature file was found.\n",
    "    Returns x and y as separate multidimensional arrays.\n",
    "    The instances in x contain only the first 6 features.\n",
    "    The ports, domain, and cipher features are stored in separate arrays for easier process in stage 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "           Path to the directory containing samples.\n",
    "    min_samples : int\n",
    "                  The number of samples each class must have at minimum (else it is pruned).\n",
    "    max_samples : int\n",
    "                  Stop loading samples for a class when this number is reached.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features_misc : numpy array\n",
    "                    Traffic statistical features\n",
    "    features_ports : numpy array\n",
    "                     Vectorized word-bags (e.g., counts) for ports\n",
    "    features_domains : numpy array\n",
    "                       Vectorized word-bags (e.g., counts) for domains\n",
    "    features_ciphers : numpy array\n",
    "                       Vectorized word-bags (e.g., counts) for ciphers\n",
    "    labels : numpy array\n",
    "             (numerical) Labels for all samples in the dataset\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    x_p = []\n",
    "    x_d = []\n",
    "    x_c = []\n",
    "    y = []\n",
    "\n",
    "    port_dict = dict()\n",
    "    domain_set = set()\n",
    "    cipher_set = set()\n",
    "\n",
    "    # Create paths and do instance count filtering\n",
    "    f_paths = []\n",
    "    f_counts = dict()\n",
    "    for rt, dirs, files in os.walk(root):\n",
    "        for f_name in files:\n",
    "            path = os.path.join(rt, f_name)\n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            name = os.path.basename(path)\n",
    "            if name.startswith(\"features\") and name.endswith(\".json\"):\n",
    "                f_paths.append((path, label, name))\n",
    "                f_counts[label] = 1 + f_counts.get(label, 0)\n",
    "\n",
    "    # Load Samples\n",
    "    processed_counts = {label: 0 for label in f_counts.keys()}\n",
    "    for fpath in tqdm.tqdm(f_paths):  # Enumerate all sample files\n",
    "        path = fpath[0]\n",
    "        label = fpath[1]\n",
    "        if f_counts[label] < min_samples:\n",
    "            continue\n",
    "        if processed_counts[label] >= max_samples:\n",
    "            continue  # Limit\n",
    "        processed_counts[label] += 1\n",
    "        with open(path, \"r\") as fp:\n",
    "            features = json.load(fp)\n",
    "            instance = [features[\"flow_volume\"],\n",
    "                        features[\"flow_duration\"],\n",
    "                        features[\"flow_rate\"],\n",
    "                        features[\"sleep_time\"],\n",
    "                        features[\"dns_interval\"],\n",
    "                        features[\"ntp_interval\"]]\n",
    "            x.append(instance)\n",
    "            x_p.append(list(features[\"ports\"]))\n",
    "            x_d.append(list(features[\"domains\"]))\n",
    "            x_c.append(list(features[\"ciphers\"]))\n",
    "            y.append(label)\n",
    "            domain_set.update(list(features[\"domains\"]))\n",
    "            cipher_set.update(list(features[\"ciphers\"]))\n",
    "            for port in set(features[\"ports\"]):\n",
    "                port_dict[port] = 1 + port_dict.get(port, 0)\n",
    "\n",
    "    # Prune rarely seen ports\n",
    "    port_set = set()\n",
    "    for port in port_dict.keys():\n",
    "        if port_dict[port] > PORT_COUNT:  # Filter out ports that are rarely seen to reduce feature dimensionality\n",
    "            port_set.add(port)\n",
    "\n",
    "    # Map to word-bag\n",
    "    print(\"Generating word-bags ... \")\n",
    "    for i in tqdm.tqdm(range(len(y))):\n",
    "        x_p[i] = list(map(lambda x: x_p[i].count(x), port_set))\n",
    "        x_d[i] = list(map(lambda x: x_d[i].count(x), domain_set))\n",
    "        x_c[i] = list(map(lambda x: x_c[i].count(x), cipher_set))\n",
    "\n",
    "    return np.array(x).astype(float), np.array(x_p), np.array(x_d), np.array(x_c), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1668706870607,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "_EPnl1blOyuV"
   },
   "outputs": [],
   "source": [
    "def classify_bayes(x_tr, y_tr, x_ts, y_ts):\n",
    "    \"\"\"\n",
    "    Use a multinomial naive bayes classifier to analyze the 'bag of words' seen in the ports/domain/ciphers features.\n",
    "    Returns the prediction results for the training and testing datasets as an array of tuples in which each row\n",
    "    represents a data instance and each tuple is composed as the predicted class and the confidence of prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_tr : numpy array\n",
    "           Array containing training samples.\n",
    "    y_tr : numpy array\n",
    "           Array containing training labels.\n",
    "    x_ts : numpy array\n",
    "           Array containing testing samples.\n",
    "    y_ts : numpy array\n",
    "           Array containing testing labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    c_tr : numpy array\n",
    "           Prediction results for training samples.\n",
    "    c_ts : numpy array\n",
    "           Prediction results for testing samples.\n",
    "    \"\"\"\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_tr, y_tr)\n",
    "\n",
    "    # Produce class and confidence for training samples\n",
    "    c_tr = classifier.predict_proba(x_tr)\n",
    "    c_tr = [(np.argmax(instance), max(instance)) for instance in c_tr]\n",
    "\n",
    "    # Produce class and confidence for testing samples\n",
    "    c_ts = classifier.predict_proba(x_ts)\n",
    "    c_ts = [(np.argmax(instance), max(instance)) for instance in c_ts]\n",
    "\n",
    "    return c_tr, c_ts\n",
    "\n",
    "\n",
    "def do_stage_0(xp_tr, xp_ts, xd_tr, xd_ts, xc_tr, xc_ts, y_tr, y_ts):\n",
    "    \"\"\"\n",
    "    Perform stage 0 of the classification procedure:\n",
    "        process each multinomial feature using naive bayes\n",
    "        return the class prediction and confidence score for each instance feature\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xp_tr : numpy array\n",
    "           Array containing training (port) samples.\n",
    "    xp_ts : numpy array\n",
    "           Array containing testing (port) samples.\n",
    "    xd_tr : numpy array\n",
    "           Array containing training (port) samples.\n",
    "    xd_ts : numpy array\n",
    "           Array containing testing (port) samples.\n",
    "    xc_tr : numpy array\n",
    "           Array containing training (port) samples.\n",
    "    xc_ts : numpy array\n",
    "           Array containing testing (port) samples.\n",
    "    y_tr : numpy array\n",
    "           Array containing training labels.\n",
    "    y_ts : numpy array\n",
    "           Array containing testing labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res_p_tr : numpy array\n",
    "               Prediction results for training (port) samples.\n",
    "    res_p_ts : numpy array\n",
    "               Prediction results for testing (port) samples.\n",
    "    res_d_tr : numpy array\n",
    "               Prediction results for training (domains) samples.\n",
    "    res_d_ts : numpy array\n",
    "               Prediction results for testing (domains) samples.\n",
    "    res_c_tr : numpy array\n",
    "               Prediction results for training (cipher suites) samples.\n",
    "    res_c_ts : numpy array\n",
    "               Prediction results for testing (cipher suites) samples.\n",
    "    \"\"\"\n",
    "    # Perform multinomial classification on bag of ports\n",
    "    res_p_tr, res_p_ts = classify_bayes(xp_tr, y_tr, xp_ts, y_ts)\n",
    "\n",
    "    # Perform multinomial classification on domain names\n",
    "    res_d_tr, res_d_ts = classify_bayes(xd_tr, y_tr, xd_ts, y_ts)\n",
    "\n",
    "    # Perform multinomial classification on cipher suites\n",
    "    res_c_tr, res_c_ts = classify_bayes(xc_tr, y_tr, xc_ts, y_ts)\n",
    "\n",
    "    return res_p_tr, res_p_ts, res_d_tr, res_d_ts, res_c_tr, res_c_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8D3Fj0EpLZ6"
   },
   "source": [
    "Your primary goal for this assignment is to implement your own versions of the decision tree and random forest algorithms to replace the scikit-learn implementation currently in use for stage-1 classification. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1668706870607,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "2bOA09maRAg9"
   },
   "outputs": [],
   "source": [
    "def gini(labels1, labels2, classnum):\n",
    "  prop1 = np.count_nonzero(labels1 == classnum)/len(labels1)\n",
    "  invprop1 = 1 - prop1\n",
    "  prop2 = np.count_nonzero(labels2 == classnum)/len(labels2)\n",
    "  invprop2 = 1 - prop2\n",
    "  totallen = len(labels1) + len(labels2)\n",
    "  gini1 = (1 - (prop1*prop1 + invprop1*invprop1)) * (len(labels1)/totallen)\n",
    "  gini2 = (1 - (prop2*prop2 + invprop2*invprop2)) * (len(labels2)/totallen)\n",
    "  return gini1 + gini2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1668706870607,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "ZUhwqYVtU1e-"
   },
   "outputs": [],
   "source": [
    "def splitfinder(labels):\n",
    "  ginis = {}\n",
    "  classes = np.unique(labels)\n",
    "  #For starters, split will be from 0.1 to 0.9\n",
    "  i = 0.1\n",
    "  while i < 1:\n",
    "    splitnum = int(len(labels)*i)\n",
    "    set1 = labels[0:splitnum]\n",
    "    set2 = labels[splitnum:]\n",
    "    ginisum = 0\n",
    "    for c in classes:\n",
    "      ginisum = ginisum + gini(set1, set2, c)\n",
    "    ginis[i] = ginisum\n",
    "    i+=0.1\n",
    "  return min(ginis, key=ginis.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAO0loIyCKHd"
   },
   "source": [
    "Working Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1668706870608,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "LeMUBL7CCI25"
   },
   "outputs": [],
   "source": [
    "def gini_impurity(groups, classes):\n",
    "  n = sum(len(group) for group in groups)\n",
    "  impurity = 0.0\n",
    "  for group in groups:\n",
    "    if len(group) == 0:\n",
    "      pass\n",
    "    else:\n",
    "      g_score = 0.0\n",
    "      for c in classes:\n",
    "        p = [row[-1] for row in group].count(c) / len(group)\n",
    "        g_score += p * p\n",
    "      impurity = impurity + (1-g_score)*(len(group)/n)\n",
    "  return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1668706870608,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "hkYHnRDzVr4F"
   },
   "outputs": [],
   "source": [
    "def split(value, feature, data):\n",
    "  yes, no = [],[]\n",
    "  for d in data:\n",
    "    if d[feature] < value:\n",
    "      no.append(d)\n",
    "    else:\n",
    "      yes.append(d)\n",
    "  return no,yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1668706870608,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "t8N0gqBOL_94"
   },
   "outputs": [],
   "source": [
    "def splitter(data, classes):\n",
    "  index, value, score, groups = 0, 0, 2.0, None\n",
    "  i = 0\n",
    "  checked_feats = []\n",
    "  while i < len(data[0])-1:\n",
    "    for d in data:\n",
    "      if d[i] not in checked_feats:\n",
    "        checked_feats.append(d[i])\n",
    "        groups = split(d[i], i, data)\n",
    "        gini = gini_impurity(groups, classes)\n",
    "        if gini < score:\n",
    "          index, value, score, groups = i, d[i], gini, groups\n",
    "    i+=1\n",
    "    checked_feats = []\n",
    "  return (index, value, score, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1668706870609,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "71lGTdp8B0j3"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "  def __init__(self, data):\n",
    "    self.left = None\n",
    "    self.right = None\n",
    "    self.data = data\n",
    "    self.spliton = None\n",
    "    self.splitthreshold = None\n",
    "    self.purity = 2.0\n",
    "  \n",
    "  def setchildren(self, feature, value, leftnode, rightnode):\n",
    "    self.left = leftnode\n",
    "    self.right = rightnode\n",
    "    self.spliton = feature\n",
    "    self.splitthreshold = value\n",
    "  \n",
    "  def getleft(self):\n",
    "    return self.left\n",
    "\n",
    "  def getright(self):\n",
    "    return self.right\n",
    "\n",
    "  def getpurity(self):\n",
    "    return self.purity\n",
    "\n",
    "  def printtree(self, layer=0):\n",
    "    if self.data is not None:\n",
    "      print(\"Layer #\" + str(layer) + \": Feature \" + str(self.spliton) + \" < \" + str(self.splitthreshold))\n",
    "      layer = layer + 1\n",
    "      if self.left is not None:\n",
    "        print(\"left side of \" + str(self.spliton) + \" < \" + str(self.splitthreshold))\n",
    "        self.left.printtree(layer)\n",
    "      if self.right is not None:\n",
    "        print(\"right side of \" + str(self.spliton) + \" < \" + str(self.splitthreshold))\n",
    "        self.right.printtree(layer)\n",
    "\n",
    "  def predict(self, sample):\n",
    "    if self.left is not None and self.right is not None:\n",
    "      #print(self.spliton)\n",
    "      #print(self.splitthreshold)\n",
    "      sampleval = sample[self.spliton]\n",
    "      if sampleval < self.splitthreshold:\n",
    "        below = self.left.predict(sample)\n",
    "        return below\n",
    "      else:\n",
    "        below = self.right.predict(sample)\n",
    "        return below\n",
    "    else:\n",
    "      classcount = {}\n",
    "      for d in self.data:\n",
    "        if d[-1] not in classcount.keys():\n",
    "          classcount[d[-1]] = 1\n",
    "        else:\n",
    "          classcount[d[-1]] = classcount[d[-1]] + 1\n",
    "      return max(classcount, key=classcount.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1668706870863,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "kpMW1_KAFyhf"
   },
   "outputs": [],
   "source": [
    "def dec_tree(max_depth, min_node, data, classes, num_features, currentnode=None, featuresused=None):\n",
    "  if max_depth == 0:\n",
    "    return currentnode\n",
    "  if currentnode is None:\n",
    "    currentnode = Node(data)\n",
    "  if featuresused is None:\n",
    "    featuresused = []\n",
    "  elif len(featuresused) >= num_features:\n",
    "    return currentnode\n",
    "  bestsplit = splitter(data, classes)\n",
    "  if bestsplit[2] > currentnode.getpurity():\n",
    "    return currentnode\n",
    "  elif len(bestsplit[3][0]) == 0 or len(bestsplit[3][1]) == 0:\n",
    "    return currentnode\n",
    "  elif len(bestsplit[3][0]) < min_node or len(bestsplit[3][1]) < min_node:\n",
    "    return currentnode\n",
    "  leftnode = dec_tree(max_depth-1, min_node, bestsplit[3][0], classes, num_features, currentnode=currentnode.getleft(), featuresused=featuresused)\n",
    "  rightnode = dec_tree(max_depth-1, min_node, bestsplit[3][1], classes, num_features, currentnode=currentnode.getright(), featuresused=featuresused)\n",
    "  currentnode.setchildren(bestsplit[0], bestsplit[1], leftnode, rightnode)\n",
    "  if bestsplit[0] not in featuresused:\n",
    "    featuresused.append(bestsplit[0])\n",
    "  return currentnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1668706870864,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "5hI0Z9EVBP6R"
   },
   "outputs": [],
   "source": [
    "def random_forest(n_trees, data_frac, feature_subcount, x_tr, y_tr):\n",
    "  reformed_x = np.zeros(shape=(len(x_tr),len(x_tr[0])+1))\n",
    "  i = 0\n",
    "  classes_y = []\n",
    "  while i < len(x_tr):\n",
    "    c = y_tr[i]\n",
    "    reformed_x[i] = np.append(x_tr[i], [c], 0)\n",
    "    if c not in classes_y:\n",
    "      classes_y.append(c)\n",
    "    i+=1\n",
    "  forest = []\n",
    "  slicesize = int(len(reformed_x)*data_frac)\n",
    "  indices = np.arange(len(reformed_x))\n",
    "  for n in range(n_trees):\n",
    "    xsample = []\n",
    "    sliceindex = np.random.choice(indices, size=slicesize, replace=False)\n",
    "    for si in sliceindex:\n",
    "      xsample.append(reformed_x[si])\n",
    "    bigtree = dec_tree(3, 10, xsample, classes_y, feature_subcount)\n",
    "    bigtree.printtree()\n",
    "    forest.append(bigtree)\n",
    "  return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1668706870864,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "S2F5zXHn6kzA"
   },
   "outputs": [],
   "source": [
    "def test(x_tr, y_tr, x_ts, y_ts):\n",
    "  numcorrect = 0 \n",
    "  trees = random_forest(2, .70, 1, x_tr, y_tr)\n",
    "  print(\"len: \" + str(len(trees)))\n",
    "  for x in x_ts:\n",
    "    treepred = {}\n",
    "    for t in trees:\n",
    "      #print(t)\n",
    "      xclass = t.predict(x)\n",
    "      #print(treepred.keys())\n",
    "      if xclass not in treepred.keys():\n",
    "        treepred[xclass] = 1\n",
    "        #print(\"if\")\n",
    "      else:\n",
    "        #print(\"else\")\n",
    "        treepred[xclass] = treepred[xclass] + 1\n",
    "    #print(treepred)\n",
    "    predclass = max(treepred, key=treepred.get)\n",
    "    xdex = np.where(x_ts == x)\n",
    "    actualclass = y_ts[xdex[0]][0]\n",
    "    #print(actualclass)\n",
    "    if predclass == actualclass:\n",
    "      numcorrect = numcorrect + 1\n",
    "  return (numcorrect/len(x_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1668706870864,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     },
     "user_tz": 300
    },
    "id": "1CmqCNZqH8HT"
   },
   "outputs": [],
   "source": [
    "  def do_stage_1(X_tr, X_ts, Y_tr, Y_ts):\n",
    "    \"\"\"\n",
    "    Perform stage 1 of the classification procedure:\n",
    "        train a random forest classifier using the NB prediction probabilities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr : numpy array\n",
    "           Array containing training samples.\n",
    "    Y_tr : numpy array\n",
    "           Array containing training labels.\n",
    "    X_ts : numpy array\n",
    "           Array containing testing samples.\n",
    "    Y_ts : numpy array\n",
    "           Array containing testing labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred : numpy array\n",
    "           Final predictions on testing dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = test(X_tr, Y_tr, X_ts, Y_ts)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Tw3vuseMdZy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668707127405,
     "user_tz": 300,
     "elapsed": 256554,
     "user": {
      "displayName": "Thomas Stone",
      "userId": "18211796150091914175"
     }
    },
    "outputId": "aacafc13-c2ec-4b18-eb80-b73378cd44bc"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Load data, encode labels to numeric, and perform classification stages \n",
    "    \"\"\"\n",
    "    # load dataset\n",
    "    print(\"Loading dataset ... \")\n",
    "    X, X_p, X_d, X_c, Y = load_data(ROOT, min_samples=MIN_SAMPLES_PER_CLASS, \n",
    "                                          max_samples=MAX_SAMPLES_PER_CLASS)\n",
    "\n",
    "    # encode labels into numerical values\n",
    "    print(\"Encoding labels ... \")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(Y)\n",
    "    Y = le.transform(Y)\n",
    "\n",
    "    print(\"Dataset statistics:\")\n",
    "    print(\"\\t Classes: {}\".format(len(le.classes_)))\n",
    "    print(\"\\t Samples: {}\".format(len(Y)))\n",
    "    print(\"\\t Dimensions: \", X.shape, X_p.shape, X_d.shape, X_c.shape)\n",
    "\n",
    "    # shuffle\n",
    "    print(\"Shuffling dataset using seed {} ... \".format(SEED))\n",
    "    s = np.arange(Y.shape[0])\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(s)\n",
    "    X, X_p, X_d, X_c, Y = X[s], X_p[s], X_d[s], X_c[s], Y[s]\n",
    "\n",
    "    # split\n",
    "    print(\"Splitting dataset using train:test ratio of {}:{} ... \".format(1-int(SPLIT*10), int((SPLIT)*10)))\n",
    "    cut = int(len(Y) * SPLIT)\n",
    "    X_tr, Xp_tr, Xd_tr, Xc_tr, Y_tr = X[cut:], X_p[cut:], X_d[cut:], X_c[cut:], Y[cut:]\n",
    "    X_ts, Xp_ts, Xd_ts, Xc_ts, Y_ts = X[:cut], X_p[:cut], X_d[:cut], X_c[:cut], Y[:cut]\n",
    "\n",
    "    # perform stage 0\n",
    "    print(\"Performing Stage 0 classification ... \")\n",
    "    p_tr, p_ts, d_tr, d_ts, c_tr, c_ts = \\\n",
    "        do_stage_0(Xp_tr, Xp_ts, Xd_tr, Xd_ts, Xc_tr, Xc_ts, Y_tr, Y_ts)\n",
    "\n",
    "    # build stage 1 dataset using stage 0 results\n",
    "    # NB predictions are concatenated to the statistical attributes processed from the flows\n",
    "    X_tr_full = np.hstack((X_tr, p_tr, d_tr, c_tr))\n",
    "    X_ts_full = np.hstack((X_ts, p_ts, d_ts, c_ts))\n",
    "\n",
    "    # perform final classification\n",
    "    print(\"Performing Stage 1 classification ... \")\n",
    "    pred = do_stage_1(X_tr_full, X_ts_full, Y_tr, Y_ts)\n",
    "\n",
    "    # print classification report\n",
    "    print(\"Prediction: \"+ str(pred))\n",
    "    #print(classification_report(Y_ts, pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "12LJ-G2znS6VJKvyJq0TmYgDRW2tos2Sd",
     "timestamp": 1667579716736
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
